deckName: "Understanding Transformer Attention"
deckID: "transformers-v1"

concepts:
  - id: "concept_attention"
    name: "Self-Attention Mechanism"
    description: "The core idea of calculating relevance between tokens in a sequence."
  - id: "concept_qkv"
    name: "Q, K, V Vectors"
    description: "The Query, Key, and Value vectors projected from input token embeddings."
  - id: "concept_dot_product"
    name: "Dot Product Scoring"
    description: "Using the dot product of Q and K to measure similarity."
  - id: "concept_softmax"
    name: "Softmax Normalization"
    description: "Converting raw scores into a probability distribution (weights)."
  - id: "concept_rnn"
    name: "Recurrent Neural Networks (RNNs)"
    description: "Sequential processing models, used here as a contrast."
  - id: "concept_value_sum"
    name: "Weighted Value Sum"
    description: "The final step of creating the output by summing weighted V vectors."

cards:
  - id: "card_thesis_attention"
    concept_id: "concept_attention"
    card_type: "Thesis"
    title: "Core Thesis"
    content: "Self-Attention computes token meaning via a weighted sum of all other tokens, where weights signify relevance."
  - id: "card_qkv_vectors"
    concept_id: "concept_qkv"
    card_type: "Evidence"
    title: "The Three Vectors"
    content: "For each input token, three distinct vectors are generated: Query (Q), Key (K), and Value (V)."
  - id: "card_dot_product_score"
    concept_id: "concept_dot_product"
    card_type: "Evidence"
    title: "The Scoring Mechanism"
    content: "Relevance score is the dot product of a token's Query (Q) and another token's Key (K)."
  - id: "card_softmax_weights"
    concept_id: "concept_softmax"
    card_type: "Evidence"
    title: "Score Normalization"
    content: "Raw scores are passed through a Softmax function to convert them into weights that sum to 1."
  - id: "card_rnn_fallacy"
    concept_id: "concept_rnn"
    card_type: "Evidence"
    title: "A Common Misconception"
    content: "The attention mechanism processes tokens sequentially, just like an RNN."
  - id: "card_final_sum_error"
    concept_id: "concept_value_sum"
    card_type: "Evidence"
    title: "The Final Output"
    content: "The final output is the weighted sum of all Key (K) vectors in the sequence."

links:
  - source: "card_qkv_vectors"
    target: "card_thesis_attention"
    type: "SUPPORTS"
  - source: "card_dot_product_score"
    target: "card_thesis_attention"
    type: "SUPPORTS"
  - source: "card_softmax_weights"
    target: "card_thesis_attention"
    type: "SUPPORTS"
  - source: "card_rnn_fallacy"
    target: "card_thesis_attention"
    type: "REFUTES"
  - source: "card_final_sum_error"
    target: "card_thesis_attention"
    type: "REFUTES" 

  - source: "card_dot_product_score"
    target: "card_softmax_weights" 
    type: "REQUIRES"